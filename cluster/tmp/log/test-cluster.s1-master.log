2022-03-28 07:41:23.526 [248] main/103/init.lua I> Using advertise_uri "localhost:3302"
2022-03-28 07:41:23.526 [248] main/103/init.lua I> Membership encryption enabled
2022-03-28 07:41:23.528 [248] main/103/init.lua I> Probe uri was successful
2022-03-28 07:41:23.528 [248] main/103/init.lua I> Membership BROADCAST sent to 127.0.0.1:3303
2022-03-28 07:41:23.529 [248] main/103/init.lua I> Membership BROADCAST sent to 172.27.255.255:3303
2022-03-28 07:41:23.532 [248] main/103/init.lua I> Membership BROADCAST sent to 127.0.0.1:3301
2022-03-28 07:41:23.533 [248] main/103/init.lua I> Membership BROADCAST sent to 172.27.255.255:3301
2022-03-28 07:41:23.533 [248] main/103/init.lua I> Membership BROADCAST sent to 127.0.0.1:3302
2022-03-28 07:41:23.534 [248] main/103/init.lua I> Membership BROADCAST sent to 172.27.255.255:3302
2022-03-28 07:41:23.546 [248] main/107/http/0.0.0.0:8082 I> started
2022-03-28 07:41:23.547 [248] main/103/init.lua I> Listening HTTP on 0.0.0.0:8082
2022-03-28 07:41:23.552 [248] main/108/console/unix/:/tmp/run/test-cluster.s1-master.control I> started
2022-03-28 07:41:24.044 [248] main/103/init.lua I> "tuple.keydef" module is not found. Built-in "key_def" is used
2022-03-28 07:41:24.094 [248] main/103/init.lua I> "tuple.merger" module is not found. Built-in "merger" is used
2022-03-28 07:41:24.168 [248] main/109/remote_control/0.0.0.0:3302 I> started
2022-03-28 07:41:24.169 [248] main/103/init.lua I> Remote control bound to 0.0.0.0:3302
2022-03-28 07:41:24.170 [248] main/103/init.lua I> Remote control ready to accept connections
2022-03-28 07:41:24.170 [248] main/103/init.lua I> Instance state changed:  -> Unconfigured
2022-03-28 07:41:24.172 [248] main/103/init.lua I> Cartridge 2.7.3
2022-03-28 07:41:24.173 [248] main/103/init.lua I> server alias s1-master
2022-03-28 07:41:24.174 [248] main/103/init.lua I> advertise uri localhost:3302
2022-03-28 07:41:24.174 [248] main/103/init.lua I> working directory /tmp/data/test-cluster.s1-master
2022-03-28 07:41:24.175 [248] main C> entering the event loop
2022-03-28 07:41:34.240 [248] main/111/console/unix/: twophase.lua:497 W> Updating config clusterwide...
2022-03-28 07:41:34.241 [248] main/111/console/unix/: twophase.lua:373 W> (2PC) patch_clusterwide upload phase...
2022-03-28 07:41:34.259 [248] main/111/console/unix/: twophase.lua:386 W> (2PC) patch_clusterwide prepare phase...
2022-03-28 07:41:34.265 [248] main/111/console/unix/: twophase.lua:395 W> Prepared for patch_clusterwide at localhost:3301
2022-03-28 07:41:34.265 [248] main/111/console/unix/: twophase.lua:395 W> Prepared for patch_clusterwide at localhost:3302
2022-03-28 07:41:34.265 [248] main/111/console/unix/: twophase.lua:395 W> Prepared for patch_clusterwide at localhost:3303
2022-03-28 07:41:34.266 [248] main/111/console/unix/: twophase.lua:395 W> Prepared for patch_clusterwide at localhost:3304
2022-03-28 07:41:34.266 [248] main/111/console/unix/: twophase.lua:395 W> Prepared for patch_clusterwide at localhost:3305
2022-03-28 07:41:34.267 [248] main/111/console/unix/: twophase.lua:419 W> (2PC) patch_clusterwide commit phase...
2022-03-28 07:41:34.268 [248] main/127/remote_control/127.0.0.1:38118 I> Instance state changed: Unconfigured -> BootstrappingBox
2022-03-28 07:41:34.269 [248] main/127/remote_control/127.0.0.1:38118 confapplier.lua:460 W> Calling box.cfg()...
2022-03-28 07:41:34.278 [248] main/127/remote_control/127.0.0.1:38118 C> Tarantool 2.8.3-83-ga3658df
2022-03-28 07:41:34.279 [248] main/127/remote_control/127.0.0.1:38118 C> log level 5
2022-03-28 07:41:34.280 [248] main/127/remote_control/127.0.0.1:38118 I> wal/engine cleanup is paused
2022-03-28 07:41:34.280 [248] main/127/remote_control/127.0.0.1:38118 I> mapping 268435456 bytes for memtx tuple arena...
2022-03-28 07:41:34.281 [248] main/127/remote_control/127.0.0.1:38118 I> Actual slab_alloc_factor calculated on the basis of desired slab_alloc_factor = 1.044274
2022-03-28 07:41:34.282 [248] main/127/remote_control/127.0.0.1:38118 I> mapping 134217728 bytes for vinyl tuple arena...
2022-03-28 07:41:34.301 [248] main/127/remote_control/127.0.0.1:38118 systemd.c:132 !> systemd: failed to send message: Connection refused
2022-03-28 07:41:34.302 [248] main/127/remote_control/127.0.0.1:38118 I> instance uuid 61aa4e22-fd63-43d2-bc56-ea84f3d76cd2
2022-03-28 07:41:34.302 [248] main/128/remote_control/127.0.0.1:38118 I> Cartridge 2.7.3
2022-03-28 07:41:34.303 [248] main/128/remote_control/127.0.0.1:38118 I> server alias s1-master
2022-03-28 07:41:34.304 [248] main/128/remote_control/127.0.0.1:38118 I> advertise uri localhost:3302
2022-03-28 07:41:34.305 [248] main/128/remote_control/127.0.0.1:38118 I> working directory /tmp/data/test-cluster.s1-master
2022-03-28 07:41:34.305 [248] main/127/remote_control/127.0.0.1:38118 I> tx_binary: stopped
2022-03-28 07:41:34.306 [248] main/127/remote_control/127.0.0.1:38118 I> initializing an empty data directory
2022-03-28 07:41:34.338 [248] main/127/remote_control/127.0.0.1:38118 I> assigned id 1 to replica 61aa4e22-fd63-43d2-bc56-ea84f3d76cd2
2022-03-28 07:41:34.338 [248] main/127/remote_control/127.0.0.1:38118 I> cluster uuid f9ac02c4-497c-40d4-890d-3b868c37caa1
2022-03-28 07:41:34.345 [248] snapshot/101/main I> saving snapshot `/tmp/data/test-cluster.s1-master/00000000000000000000.snap.inprogress'
2022-03-28 07:41:34.355 [248] snapshot/101/main I> done
2022-03-28 07:41:34.356 [248] main/127/remote_control/127.0.0.1:38118 systemd.c:132 !> systemd: failed to send message: Connection refused
2022-03-28 07:41:34.356 [248] main/127/remote_control/127.0.0.1:38118 I> ready to accept requests
2022-03-28 07:41:34.356 [248] main/129/gc I> wal/engine cleanup is resumed
2022-03-28 07:41:34.357 [248] main/127/remote_control/127.0.0.1:38118 I> set 'custom_proc_title' configuration option to "test-cluster@s1-master"
2022-03-28 07:41:34.357 [248] main/127/remote_control/127.0.0.1:38118 I> set 'log_level' configuration option to 5
2022-03-28 07:41:34.357 [248] main/130/checkpoint_daemon I> scheduled next checkpoint for Mon Mar 28 09:36:26 2022
2022-03-28 07:41:34.368 [248] main/127/remote_control/127.0.0.1:38118 I> set 'instance_uuid' configuration option to "61aa4e22-fd63-43d2-bc56-ea84f3d76cd2"
2022-03-28 07:41:34.373 [248] main/127/remote_control/127.0.0.1:38118 I> set 'replication_connect_quorum' configuration option to 0
2022-03-28 07:41:34.374 [248] main/127/remote_control/127.0.0.1:38118 I> set 'log_format' configuration option to "plain"
2022-03-28 07:41:34.374 [248] main/127/remote_control/127.0.0.1:38118 I> set 'replicaset_uuid' configuration option to "f9ac02c4-497c-40d4-890d-3b868c37caa1"
2022-03-28 07:41:34.377 [248] main/127/remote_control/127.0.0.1:38118 I> Making sure user "admin" exists...
2022-03-28 07:41:34.377 [248] main/127/remote_control/127.0.0.1:38118 I> Granting replication permissions to "admin"...
2022-03-28 07:41:34.378 [248] main/127/remote_control/127.0.0.1:38118 I> Setting password for user "admin" ...
2022-03-28 07:41:34.382 [248] main/127/remote_control/127.0.0.1:38118 I> Remote control stopped
2022-03-28 07:41:34.382 [248] main/109/remote_control/0.0.0.0:3302 I> stopped
2022-03-28 07:41:34.382 [248] main/127/remote_control/127.0.0.1:38118 I> tx_binary: stopped
2022-03-28 07:41:34.383 [248] main/127/remote_control/127.0.0.1:38118 I> tx_binary: bound to 0.0.0.0:3302
2022-03-28 07:41:34.383 [248] main/127/remote_control/127.0.0.1:38118 I> set 'listen' configuration option to "3302"
2022-03-28 07:41:34.383 [248] main/127/remote_control/127.0.0.1:38118 I> Instance state changed: BootstrappingBox -> ConnectingFullmesh
2022-03-28 07:41:34.384 [248] main/127/remote_control/127.0.0.1:38118 I> connecting to 2 replicas
2022-03-28 07:41:34.384 [248] main/127/remote_control/127.0.0.1:38118 C> failed to connect to 2 out of 2 replicas
2022-03-28 07:41:34.384 [248] main/127/remote_control/127.0.0.1:38118 C> leaving orphan mode
2022-03-28 07:41:34.385 [248] main/127/remote_control/127.0.0.1:38118 systemd.c:132 !> systemd: failed to send message: Connection refused
2022-03-28 07:41:34.385 [248] main/127/remote_control/127.0.0.1:38118 I> set 'replication' configuration option to ["admin@localhost:3302","admin@localhost:3303"]
2022-03-28 07:41:34.385 [248] main/127/remote_control/127.0.0.1:38118 I> Instance state changed: ConnectingFullmesh -> BoxConfigured
2022-03-28 07:41:34.385 [248] main/127/remote_control/127.0.0.1:38118 I> Instance state changed: BoxConfigured -> ConfiguringRoles
2022-03-28 07:41:34.386 [248] main/127/remote_control/127.0.0.1:38118 I> Failover disabled
2022-03-28 07:41:34.387 [248] main/127/remote_control/127.0.0.1:38118 I> Replicaset dca7d0e4-2d29-4f00-ad2f-b15461f9273b: new leader 7151d922-1035-49c3-8da8-e99c8406223c ("localhost:3301"), was nil
2022-03-28 07:41:34.387 [248] main/127/remote_control/127.0.0.1:38118 I> Replicaset f9ac02c4-497c-40d4-890d-3b868c37caa1 (me): new leader 61aa4e22-fd63-43d2-bc56-ea84f3d76cd2 (me), was nil
2022-03-28 07:41:34.388 [248] main/127/remote_control/127.0.0.1:38118 I> Replicaset eef0556e-a2b9-4edd-bb31-2506e2ddde0f: new leader d7f4147f-0621-4967-b232-d44d72d59dde ("localhost:3304"), was nil
2022-03-28 07:41:34.388 [248] main/127/remote_control/127.0.0.1:38118 I> Reconfiguring vshard.storage...
2022-03-28 07:41:34.388 [248] main/127/remote_control/127.0.0.1:38118 I> Starting configuration of replica 61aa4e22-fd63-43d2-bc56-ea84f3d76cd2
2022-03-28 07:41:34.389 [248] main/127/remote_control/127.0.0.1:38118 I> I am master
2022-03-28 07:41:34.389 [248] main/127/remote_control/127.0.0.1:38118 I> Taking on replicaset master role...
2022-03-28 07:41:34.390 [248] main/127/remote_control/127.0.0.1:38118 I> Box has been configured
2022-03-28 07:41:34.390 [248] main/127/remote_control/127.0.0.1:38118 I> Initializing schema {0.1.15.0}
2022-03-28 07:41:34.391 [248] main/146/applier/admin@localhost:3303 I> remote master 00000000-0000-0000-0000-000000000000 at 127.0.0.1:3303 running Tarantool 1.10.0
2022-03-28 07:41:34.391 [248] main/147/applier/admin@localhost:3302 I> remote master 61aa4e22-fd63-43d2-bc56-ea84f3d76cd2 at 127.0.0.1:3302 running Tarantool 2.8.3
2022-03-28 07:41:34.392 [248] main/146/applier/admin@localhost:3303 I> can't connect to master
2022-03-28 07:41:34.392 [248] main/146/applier/admin@localhost:3303 coio.cc:359 !> SystemError unexpected EOF when reading from socket, called on fd 27, aka 127.0.0.1:33140, peer of 127.0.0.1:3303: Broken pipe
2022-03-28 07:41:34.392 [248] main/146/applier/admin@localhost:3303 I> will retry every 1.00 second
2022-03-28 07:41:34.393 [248] main/147/applier/admin@localhost:3302 C> leaving orphan mode
2022-03-28 07:41:34.393 [248] main/147/applier/admin@localhost:3302 systemd.c:132 !> systemd: failed to send message: Connection refused
2022-03-28 07:41:34.397 [248] main/127/remote_control/127.0.0.1:38118 I> Upgrade vshard schema to {0.1.16.0}
2022-03-28 07:41:34.397 [248] main/127/remote_control/127.0.0.1:38118 I> Insert 'vshard_version' into _schema
2022-03-28 07:41:34.398 [248] main/127/remote_control/127.0.0.1:38118 I> Create function vshard.storage._call()
2022-03-28 07:41:34.398 [248] main/127/remote_control/127.0.0.1:38118 I> Successful vshard schema upgrade to {0.1.16.0}
2022-03-28 07:41:34.399 [248] main/148/lua I> gc_bucket_f has been started
2022-03-28 07:41:34.399 [248] main/149/lua I> recovery_f has been started
2022-03-28 07:41:34.399 [248] main/127/remote_control/127.0.0.1:38118 I> Took on replicaset master role
2022-03-28 07:41:34.401 [248] main/150/lua I> rebalancer_f has been started
2022-03-28 07:41:34.411 [248] main/127/remote_control/127.0.0.1:38118 I> Roles configuration finished
2022-03-28 07:41:34.411 [248] main/127/remote_control/127.0.0.1:38118 I> Instance state changed: ConfiguringRoles -> RolesConfigured
2022-03-28 07:41:34.412 [248] main/122/remote_control/127.0.0.1:38118 utils.c:449 E> LuajitError: builtin/socket.lua:88: attempt to use closed socket
2022-03-28 07:41:34.413 [248] main/151/localhost:3302 (net.box) I> connected to localhost:3302
2022-03-28 07:41:34.418 [248] main/152/localhost:3304 (net.box) I> connected to localhost:3304
2022-03-28 07:41:34.419 [248] main/150/vshard.rebalancer I> Total active bucket count is not equal to total. Possibly a boostrap is not finished yet. Expected 30000, but found 0
2022-03-28 07:41:34.419 [248] main/150/vshard.rebalancer I> Some buckets are not active, retry rebalancing later
2022-03-28 07:41:35.343 [248] main/134/main I> joining replica 6ac0443e-c998-462d-9077-a3bd00a625f0 at fd 15, aka 127.0.0.1:3302, peer of 127.0.0.1:38168
2022-03-28 07:41:35.344 [248] main/134/main I> initial data sent.
2022-03-28 07:41:35.345 [248] main/134/main I> assigned id 2 to replica 6ac0443e-c998-462d-9077-a3bd00a625f0
2022-03-28 07:41:35.345 [248] relay/127.0.0.1:38168/101/main I> recover from `/tmp/data/test-cluster.s1-master/00000000000000000000.xlog'
2022-03-28 07:41:35.345 [248] main/134/main I> final data sent.
2022-03-28 07:41:35.391 [248] main/134/main I> subscribed replica 6ac0443e-c998-462d-9077-a3bd00a625f0 at fd 15, aka 127.0.0.1:3302, peer of 127.0.0.1:38168
2022-03-28 07:41:35.391 [248] main/134/main I> remote vclock {1: 44} local vclock {1: 44}
2022-03-28 07:41:35.392 [248] relay/127.0.0.1:38168/101/main I> recover from `/tmp/data/test-cluster.s1-master/00000000000000000000.xlog'
2022-03-28 07:41:35.480 [248] main/111/console/unix/: twophase.lua:428 W> Committed patch_clusterwide at localhost:3301
2022-03-28 07:41:35.480 [248] main/111/console/unix/: twophase.lua:428 W> Committed patch_clusterwide at localhost:3302
2022-03-28 07:41:35.480 [248] main/111/console/unix/: twophase.lua:428 W> Committed patch_clusterwide at localhost:3303
2022-03-28 07:41:35.480 [248] main/111/console/unix/: twophase.lua:428 W> Committed patch_clusterwide at localhost:3304
2022-03-28 07:41:35.481 [248] main/111/console/unix/: twophase.lua:428 W> Committed patch_clusterwide at localhost:3305
2022-03-28 07:41:35.481 [248] main/111/console/unix/: twophase.lua:573 W> Clusterwide config updated successfully
2022-03-28 07:41:35.483 [248] main/111/console/unix/: twophase.lua:497 W> Updating config clusterwide...
2022-03-28 07:41:35.484 [248] main/111/console/unix/: twophase.lua:373 W> (2PC) patch_clusterwide upload phase...
2022-03-28 07:41:35.493 [248] main/111/console/unix/: twophase.lua:386 W> (2PC) patch_clusterwide prepare phase...
2022-03-28 07:41:35.497 [248] main/111/console/unix/: twophase.lua:395 W> Prepared for patch_clusterwide at localhost:3301
2022-03-28 07:41:35.497 [248] main/111/console/unix/: twophase.lua:395 W> Prepared for patch_clusterwide at localhost:3302
2022-03-28 07:41:35.497 [248] main/111/console/unix/: twophase.lua:395 W> Prepared for patch_clusterwide at localhost:3303
2022-03-28 07:41:35.498 [248] main/111/console/unix/: twophase.lua:395 W> Prepared for patch_clusterwide at localhost:3304
2022-03-28 07:41:35.498 [248] main/111/console/unix/: twophase.lua:395 W> Prepared for patch_clusterwide at localhost:3305
2022-03-28 07:41:35.498 [248] main/111/console/unix/: twophase.lua:419 W> (2PC) patch_clusterwide commit phase...
2022-03-28 07:41:35.499 [248] main/153/main I> Backup of active config created: "/tmp/data/test-cluster.s1-master/config.backup"
2022-03-28 07:41:35.500 [248] main/153/main I> Instance state changed: RolesConfigured -> ConfiguringRoles
2022-03-28 07:41:35.500 [248] main/153/main I> Failover disabled
2022-03-28 07:41:35.501 [248] main/153/main I> Roles configuration finished
2022-03-28 07:41:35.501 [248] main/153/main I> Instance state changed: ConfiguringRoles -> RolesConfigured
2022-03-28 07:41:35.504 [248] main/111/console/unix/: twophase.lua:428 W> Committed patch_clusterwide at localhost:3301
2022-03-28 07:41:35.504 [248] main/111/console/unix/: twophase.lua:428 W> Committed patch_clusterwide at localhost:3302
2022-03-28 07:41:35.504 [248] main/111/console/unix/: twophase.lua:428 W> Committed patch_clusterwide at localhost:3303
2022-03-28 07:41:35.505 [248] main/111/console/unix/: twophase.lua:428 W> Committed patch_clusterwide at localhost:3304
2022-03-28 07:41:35.505 [248] main/111/console/unix/: twophase.lua:428 W> Committed patch_clusterwide at localhost:3305
2022-03-28 07:41:35.505 [248] main/111/console/unix/: twophase.lua:573 W> Clusterwide config updated successfully
2022-03-28 07:41:35.616 [248] main/164/main I> Backup of active config created: "/tmp/data/test-cluster.s1-master/config.backup"
2022-03-28 07:41:35.616 [248] main/164/main I> Instance state changed: RolesConfigured -> ConfiguringRoles
2022-03-28 07:41:35.617 [248] main/164/main I> Failover disabled
2022-03-28 07:41:35.618 [248] main/164/main I> Roles configuration finished
2022-03-28 07:41:35.618 [248] main/164/main I> Instance state changed: ConfiguringRoles -> RolesConfigured
2022-03-28 07:41:36.394 [248] main/146/applier/admin@localhost:3303 I> remote master 6ac0443e-c998-462d-9077-a3bd00a625f0 at 127.0.0.1:3303 running Tarantool 2.8.3
2022-03-28 07:41:36.395 [248] main/146/applier/admin@localhost:3303 I> authenticated
2022-03-28 07:41:36.396 [248] main/146/applier/admin@localhost:3303 I> subscribed
2022-03-28 07:41:36.396 [248] main/146/applier/admin@localhost:3303 I> remote vclock {1: 15044} local vclock {1: 15044}
2022-03-28 07:41:36.396 [248] main/146/applier/admin@localhost:3303 I> RAFT: message {term: 1, state: follower} from 2
2022-03-28 07:41:36.399 [248] main/165/applierw/admin@localhost:3303 C> leaving orphan mode
2022-03-28 07:41:36.400 [248] main/165/applierw/admin@localhost:3303 systemd.c:132 !> systemd: failed to send message: Connection refused
2022-03-28 07:41:44.423 [248] main/150/vshard.rebalancer I> The cluster is balanced ok. Schedule next rebalancing after 3600.000000 seconds
2022-03-28 07:41:45.716 [248] main/166/main I> Backup of active config created: "/tmp/data/test-cluster.s1-master/config.backup"
2022-03-28 07:41:45.716 [248] main/166/main I> Instance state changed: RolesConfigured -> ConfiguringRoles
2022-03-28 07:41:45.717 [248] main/166/main I> Failover disabled
2022-03-28 07:41:45.719 [248] main/166/main I> Roles configuration finished
2022-03-28 07:41:45.719 [248] main/166/main I> Instance state changed: ConfiguringRoles -> RolesConfigured
2022-03-28 07:41:45.779 [248] main C> got signal 15 - Terminated
2022-03-28 07:41:45.780 [248] main/167/trigger_fiber0 I> Starting reconfiguration of replica 61aa4e22-fd63-43d2-bc56-ea84f3d76cd2
2022-03-28 07:41:45.780 [248] main/167/trigger_fiber0 I> Resigning from the replicaset master role...
2022-03-28 07:41:45.781 [248] main/167/trigger_fiber0 I> Box has been configured
2022-03-28 07:41:45.781 [248] main/168/lua I> Old replicaset and replica objects are outdated.
2022-03-28 07:41:45.781 [248] main/167/trigger_fiber0 I> GC stopped
2022-03-28 07:41:45.782 [248] main/167/trigger_fiber0 I> Recovery stopped
2022-03-28 07:41:45.782 [248] main/167/trigger_fiber0 I> Resigned from the replicaset master role
2022-03-28 07:41:45.782 [248] main/167/trigger_fiber0 I> Rebalancer location has changed to nil
2022-03-28 07:41:45.803 [248] main/167/trigger_fiber0 I> disconnected from localhost:3302
2022-03-28 07:41:45.804 [248] main I> tx_binary: stopped
